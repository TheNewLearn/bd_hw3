{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bd_hw3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMk3VzYTVtaMkkD25Y9EH9W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheNewLearn/bd_hw3/blob/main/bd_hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZXChez2ctK8",
        "outputId": "0799dcae-d6a7-4e57-c74b-a07a99eb7056"
      },
      "source": [
        "!apt-get -y install openjdk-8-jre-headless\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('hw3').config(\"spark.jars.packages\",\"com.databricks:spark-xml_2.12:0.13.0\").getOrCreate()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-dejavu-extra fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 28.2 MB of archives.\n",
            "After this operation, 104 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u292-b10-0ubuntu1~18.04 [28.2 MB]\n",
            "Fetched 28.2 MB in 2s (16.2 MB/s)\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 41.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=66a082a6c662a1ca9c67752cfb50b91bd852c4e04ff3db7ea34ff2f33373a991\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr8ETrcGhG4O",
        "outputId": "6e63dab7-48ef-454c-ec5b-e8e045c3c3ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "path = \"/content/drive/My Drive/bd_hw3/reut2-000.sgm\""
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZowXjx4hg6c"
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "def k_shingling(text,k):\n",
        "  t = text.split()\n",
        "  ls = [' '.join(t[i:i+k])\n",
        "                     for i in range(len(t) - k + 1)]\n",
        "  return ls\n",
        "\n",
        "doc = []\n",
        "\n",
        "for i in range(0,22):\n",
        "  doc.append([])\n",
        "  if i < 10:\n",
        "    dataset2 = spark.read.format('xml')\\\n",
        "            .option('rootTag',\"\")\\\n",
        "            .option('rowTag','BODY')\\\n",
        "            .load(\"/content/drive/My Drive/bd_hw3/reut2-00\"+str(i)+\".sgm\")\n",
        "  elif i >= 10:\n",
        "    dataset2 = spark.read.format('xml')\\\n",
        "            .option('rootTag',\"\")\\\n",
        "            .option('rowTag','BODY')\\\n",
        "            .load(\"/content/drive/My Drive/bd_hw3/reut2-0\"+str(i)+\".sgm\")\n",
        "  newdataset2 = dataset2.select(\"_corrupt_record\", f.regexp_replace(f.col(\"_corrupt_record\"), \"[\\$#,<BODY></BODY>;'\\''/']\", \"\").alias(\"replaced\"))\n",
        "  content2 = newdataset2.rdd.map(lambda x:x[1].replace(\"\\n\",\"\")).map(lambda x:x.replace(\"&3\",\"\")).map(lambda x:x.replace(\"  \",\"\")).map(lambda x:x.replace(\"&lt\",\"\"))\n",
        "  doc[i] += content2.collect()\n",
        "\n",
        "\n",
        "\n",
        "def compare(x,ls):\n",
        "  for i in ls:\n",
        "    if x in i:\n",
        "      return \"1\"\n",
        "    else:\n",
        "      return \"0\"\n",
        "\n",
        "allks = []\n",
        "\n",
        "for i in range(0,22):\n",
        "  if i < 10:\n",
        "    dataset = spark.read.format('xml')\\\n",
        "            .option('rootTag',\"\")\\\n",
        "            .option('rowTag','BODY')\\\n",
        "            .load(\"/content/drive/My Drive/bd_hw3/reut2-00\"+str(i)+\".sgm\")\n",
        "  elif i >= 10:\n",
        "    dataset = spark.read.format('xml')\\\n",
        "            .option('rootTag',\"\")\\\n",
        "            .option('rowTag','BODY')\\\n",
        "            .load(\"/content/drive/My Drive/bd_hw3/reut2-0\"+str(i)+\".sgm\")\n",
        "\n",
        "  newdataset = dataset.select(\"_corrupt_record\", f.regexp_replace(f.col(\"_corrupt_record\"), \"[\\$#,<BODY></BODY>;'\\''/']\", \"\").alias(\"replaced\"))\n",
        "  content = newdataset.rdd.map(lambda x:x[1].replace(\"\\n\",\"\")).map(lambda x:x.replace(\"&3\",\"\")).map(lambda x:x.replace(\"  \",\"\")).map(lambda x:x.replace(\"&lt\",\"\"))\n",
        "  content = content.flatMap(lambda x: k_shingling(x,3))\n",
        "  allks += content.collect()"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq7vaESa9FVH",
        "outputId": "23da3b49-3e4d-4c43-b589-e23f649a6057"
      },
      "source": [
        "#task1 matrix\n",
        "col = []\n",
        "col.append(\"    \")\n",
        "data2 = []\n",
        "for column in range(len(doc)):\n",
        "  col.append(\"Doc\"+str(column))\n",
        "\n",
        "for row in range(0,1000):\n",
        "  data2.append([])\n",
        "  data2[row].append(str(allks[row]))\n",
        "  for column in range(len(doc)):\n",
        "    match = 0\n",
        "    for index in doc[column]:\n",
        "      if allks[row] in index:\n",
        "        match = 1\n",
        "    data2[row].append(str(match))\n",
        "\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data2)\n",
        "\n",
        "df = spark.createDataFrame(rdd).toDF(col[0],col[1],col[2],col[3],col[4],col[5],col[6],col[7],col[8],col[9],col[10],col[11],col[12],col[13],col[14],col[15],col[16],col[17],col[18],col[19],col[20],col[21],col[22])\n",
        "\n",
        "df.show(100)\n"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|                    |Doc0|Doc1|Doc2|Doc3|Doc4|Doc5|Doc6|Doc7|Doc8|Doc9|Doc10|Doc11|Doc12|Doc13|Doc14|Doc15|Doc16|Doc17|Doc18|Doc19|Doc20|Doc21|\n",
            "+--------------------+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|Showers continued...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|continued through...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "| throughout the week|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|      the week inthe|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|     week inthe ahia|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    inthe ahia cocoa|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|     ahia cocoa zone|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|cocoa zone allevi...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|zone alleviating the|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|alleviating the d...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|   the drought since|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|drought since ear...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|since earlyJanuar...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|earlyJanuary and ...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|and improving pro...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|improving prospec...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|   prospects for the|   1|   0|   1|   0|   1|   1|   1|   0|   1|   0|    0|    1|    0|    0|    1|    1|    1|    1|    0|    0|    1|    0|\n",
            "|      for the coming|   1|   0|   1|   1|   1|   1|   1|   0|   1|   1|    1|    0|    1|    0|    0|    1|    0|    0|    1|    0|    0|    0|\n",
            "|the coming tempor...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|coming temporaoal...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|temporaoalthough ...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|normal humidity l...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|humidity levels have|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|     levels have not|   1|   1|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|       have not been|   1|   1|   1|   0|   1|   1|   1|   1|   1|   1|    1|    1|    1|    0|    0|    1|    1|    1|    1|    1|    1|    1|\n",
            "|not been restored...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|been restoredComi...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|restoredComissari...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|       Smith said in|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|         said in its|   1|   1|   1|   1|   1|   1|   1|   1|   1|   1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    0|    0|\n",
            "|       in its weekly|   1|   0|   0|   0|   0|   0|   0|   0|   0|   1|    1|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|\n",
            "|its weekly review...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|weekly review.The...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|review.The dry pe...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    dry period means|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    period means the|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  means the temporao|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|   the temporao will|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    temporao will be|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|        will be late|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|        be late this|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|late this year.Ar...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|this year.Arrival...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|year.Arrivals for...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|        for the week|   1|   1|   1|   1|   0|   0|   1|   1|   1|   0|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    0|    1|\n",
            "|      the week ended|   1|   1|   1|   1|   1|   1|   1|   1|   1|   1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    0|    1|\n",
            "| week ended February|   1|   0|   1|   1|   1|   1|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|   ended February 22|   1|   0|   0|   0|   1|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|\n",
            "|    February 22 were|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|      22 were 155221|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  were 155221 bagsof|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    155221 bagsof 60|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|     bagsof 60 kilos|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|     60 kilos making|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|      kilos making a|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "| making a cumulative|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  a cumulative total|   1|   0|   0|   0|   0|   1|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|cumulative total for|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|\n",
            "|       total for the|   1|   0|   0|   0|   0|   0|   0|   0|   0|   1|    0|    1|    0|    0|    0|    1|    1|    0|    0|    0|    0|    0|\n",
            "|      for the season|   1|   0|   0|   0|   1|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    1|    0|    0|\n",
            "|       the season of|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|   season of 5.93mln|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  of 5.93mln against|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|5.93mln against 5.81|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|     against 5.81 at|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|         5.81 at the|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|         at the same|   1|   1|   1|   1|   1|   1|   1|   1|   1|   1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    1|    0|\n",
            "|      the same stage|   1|   0|   0|   0|   0|   1|   0|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    1|\n",
            "|     same stage last|   1|   0|   0|   0|   0|   1|   0|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    1|\n",
            "|    stage last year.|   1|   0|   0|   0|   0|   1|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    last year. Again|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|      year. Again it|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  Again it seemsthat|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  it seemsthat cocoa|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|seemsthat cocoa d...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|cocoa delivered e...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|delivered earlier on|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|earlier on consig...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  on consignment was|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|consignment was i...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|     was included in|   1|   1|   0|   0|   1|   1|   1|   0|   0|   0|    0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|\n",
            "|included in thear...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|in thearrivals fi...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|thearrivals figur...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|figures.Comissari...|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    Smith said there|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|       said there is|   1|   1|   1|   1|   1|   1|   1|   1|   1|   0|    1|    1|    1|    0|    1|    1|    0|    1|    1|    0|    1|    0|\n",
            "|      there is still|   1|   0|   1|   1|   1|   0|   1|   1|   0|   0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|       is still some|   1|   0|   0|   0|   0|   0|   0|   0|   1|   0|    0|    0|    0|    0|    0|    1|    0|    0|    0|    0|    0|    0|\n",
            "|    still some doubt|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|       some doubt as|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|         doubt as to|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|       as to howmuch|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|      to howmuch old|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|    howmuch old crop|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|      old crop cocoa|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|       crop cocoa is|   1|   0|   1|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|      cocoa is still|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  is still available|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "|  still available as|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|\n",
            "+--------------------+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "only showing top 100 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HBIpRsg_pVa",
        "outputId": "1c914bce-ea75-49aa-f33d-ef03d199a5b4"
      },
      "source": [
        "import random\n",
        "\n",
        "maxShingleID = 2**32-1\n",
        "\n",
        "def pickRandomCoeffs(k):\n",
        "  # Create a list of 'k' random values.\n",
        "  randList = []\n",
        "  \n",
        "  while k > 0:\n",
        "    # Get a random shingle ID.\n",
        "    randIndex = random.randint(0, maxShingleID) \n",
        "  \n",
        "    # Ensure that each random number is unique.\n",
        "    while randIndex in randList:\n",
        "      randIndex = random.randint(0, maxShingleID) \n",
        "    \n",
        "    # Add the random number to the list.\n",
        "    randList.append(randIndex)\n",
        "    k = k - 1\n",
        "    \n",
        "  return randList\n",
        "\n",
        "\n",
        "\n",
        "print(pickRandomCoeffs(10))\n",
        "print(data2)\n"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1669360650, 3697042388, 1045979734, 227353940, 1219840041, 3587886447, 3453852794, 270828717, 327011366, 3303885087]\n",
            "[['Showers continued throughout', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['continued throughout the', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['throughout the week', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['the week inthe', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['week inthe ahia', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['inthe ahia cocoa', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['ahia cocoa zone', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['cocoa zone alleviating', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['zone alleviating the', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['alleviating the drought', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['the drought since', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['drought since earlyJanuary', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['since earlyJanuary and', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['earlyJanuary and improving', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['and improving prospects', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['improving prospects for', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['prospects for the', '1', '0', '1', '0', '1', '1', '1', '0', '1', '0', '0', '1', '0', '0', '1', '1', '1', '1', '0', '0', '1', '0'], ['for the coming', '1', '0', '1', '1', '1', '1', '1', '0', '1', '1', '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0'], ['the coming temporaoalthough', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['coming temporaoalthough normal', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax1Vz0HrZPU-",
        "outputId": "25ef8b7a-9ae1-443c-984d-106861fb4458"
      },
      "source": [
        "import random\n",
        "maxnum = 2**32-1\n",
        "\n",
        "def minhash(k):\n",
        "#k hash function\n",
        "  hn = []\n",
        "  for i in range(0,len(data2)):\n",
        "    hn.append([])\n",
        "    for j in range(k):\n",
        "      a = random.randint(0, maxnum)\n",
        "      b  = random.randint(0, maxnum)\n",
        "      rowindex =  (a*i+b)%maxnum\n",
        "      hn[i].append(rowindex)\n",
        "  ms = []\n",
        "  for k in range(len(hn[0])):\n",
        "    ms.append([])\n",
        "    for i in range(0, len(data2[0])):\n",
        "      min = maxnum\n",
        "      for row in range(len(data2)):\n",
        "        if data2[row][i] == \"1\":\n",
        "          if min > hn[row][k]:\n",
        "            min = hn[row][k]\n",
        "        else:\n",
        "          continue\n",
        "      ms[k].append(min)\n",
        "\n",
        "    \n",
        "  return ms\n",
        "\n",
        "\n",
        "a = minhash(2)\n",
        "c = minhash(1)\n",
        "\n",
        "for i in range(len(a)):\n",
        "  for j in a[i]:\n",
        "    print(j,end=\" \")\n",
        "  print(\" \")\n",
        "\n",
        "print(len(c))\n",
        "\n",
        "\n"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4294967295 14664604 102817572 75908391 75908391 68268447 75908391 23175880 75908391 75908391 68268447 68268447 75908391 75908391 75908391 102817572 23175880 23175880 75908391 102817572 75908391 75908391 102817572  \n",
            "4294967295 931666 4646635 4646635 4646635 5776147 20177522 4646635 4646635 4646635 11675924 4646635 4646635 4646635 4646635 4646635 4646635 4646635 4646635 4646635 4646635 39009024 4646635  \n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1jhDfvKzjyw",
        "outputId": "62f3f335-90d7-4215-e9b0-d044cf25c8c8"
      },
      "source": [
        "\n",
        "\n",
        "for i in range(0,len(data2)):\n",
        "  minindex = maxnum\n",
        "  for j in range(0,len(data2[i])):\n",
        "    if data2[i][j] == \"1\":\n",
        "      for k in range(0,len(a[i])):\n",
        "        if a[i][k] < minindex:\n",
        "          minindex = a[i][k]\n",
        "      \n",
        "      print(minindex,end=\" \")\n",
        "  print(\" \")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  \n",
            "2  \n",
            "3  \n",
            "4  \n",
            "5  \n",
            "6  \n",
            "7  \n",
            "8  \n",
            "9  \n",
            "10  \n",
            "11  \n",
            "12  \n",
            "13  \n",
            "14  \n",
            "15  \n",
            "16  \n",
            "17 17 17 17 17 17 17 17 17 17 17 17  \n",
            "18 18 18 18 18 18 18 18 18 18 18 18  \n",
            "19  \n",
            "0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iKdFagY5b0n",
        "outputId": "df66db81-46c7-4dde-8514-69baf0717c88"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "d = [[1,0,0,1],\n",
        "     [0,0,1,0],\n",
        "     [0,1,0,1],\n",
        "     [1,0,1,1],\n",
        "     [0,0,1,0]]\n",
        "\n",
        "\n",
        "f = [[1,1],\n",
        "     [2,4],\n",
        "     [3,2],\n",
        "     [4,0],\n",
        "     [0,3]]\n",
        "\n",
        "sc = []\n",
        "\n",
        "\n",
        "\n",
        "min = 20\n",
        "\n",
        "\n",
        "for k in range(len(f[0])):\n",
        "  sc.append([])\n",
        "  for i in range(0, len(d[0])):\n",
        "    min = 20\n",
        "    for row in range(len(d)):\n",
        "      if d[row][i] == 1:\n",
        "        if min > f[row][k]:\n",
        "          min = f[row][k]\n",
        "      else:\n",
        "        continue\n",
        "    sc[k].append(min)\n",
        "  \n",
        "\n",
        "\n",
        "print(sc)     \n",
        "\n",
        "    \n"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 3, 0, 1], [0, 2, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhKPr3VK4QPD",
        "outputId": "06f88007-d815-4905-b723-8d8d19bf2ebe"
      },
      "source": [
        "\n",
        "def lsh(b,r,sign):\n",
        "  band = []\n",
        "  for i in range(0,b):\n",
        "    band.append([])\n",
        "    for k in range(0, len(sign[0])):\n",
        "      for row in range(0,len(sign)):\n",
        "        band[i].append(sign[row][k])\n",
        "  return band\n",
        "\n",
        "print(a[0])\n",
        "#a = lsh(2,2,c)\n",
        "b = lsh(2,2,c)\n",
        "\n",
        "print(a)"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4294967295, 14664604, 102817572, 75908391, 75908391, 68268447, 75908391, 23175880, 75908391, 75908391, 68268447, 68268447, 75908391, 75908391, 75908391, 102817572, 23175880, 23175880, 75908391, 102817572, 75908391, 75908391, 102817572]\n",
            "[[4294967295, 14664604, 102817572, 75908391, 75908391, 68268447, 75908391, 23175880, 75908391, 75908391, 68268447, 68268447, 75908391, 75908391, 75908391, 102817572, 23175880, 23175880, 75908391, 102817572, 75908391, 75908391, 102817572], [4294967295, 931666, 4646635, 4646635, 4646635, 5776147, 20177522, 4646635, 4646635, 4646635, 11675924, 4646635, 4646635, 4646635, 4646635, 4646635, 4646635, 4646635, 4646635, 4646635, 4646635, 39009024, 4646635]]\n"
          ]
        }
      ]
    }
  ]
}